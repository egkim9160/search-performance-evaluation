# ê²€ìƒ‰ í’ˆì§ˆ í‰ê°€ ì™„ì „ íŒŒì´í”„ë¼ì¸

## ì „ì²´ ì›Œí¬í”Œë¡œìš°

```
Step 01: ê²€ìƒ‰ ë¡œê·¸ ìˆ˜ì§‘
   â†“
Step 02: HEAD/TAIL ì¿¼ë¦¬ ì„ ì •
   â†“
Step 03: OpenSearch ê²€ìƒ‰ ì‹¤í–‰ (Lexical/Semantic)
   â†“
Step 04: Depth-K Pooling (ê²°ê³¼ í†µí•©)
   â†“
Step 05: Poolì„ OpenSearchì— ì—…ë¡œë“œ
   â†“
Step 06: AI ê¸°ë°˜ Relevance Labeling
   â†“
Step 07: í‰ê°€ ì§€í‘œ ê³„ì‚° (nDCG, Recall, MRR)
   â†“
Step 08: ê²°ê³¼ ì‹œê°í™” ë° ë¦¬í¬íŠ¸
```

---

## Step 01: ê²€ìƒ‰ ë¡œê·¸ ìˆ˜ì§‘

MySQLì—ì„œ ê²€ìƒ‰ ë¡œê·¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.

```bash
python process/01.fetch_search_logs.py \
  --start_date 2024-05-01 \
  --end_date 2025-10-30 \
  --out_dir data/raw
```

**ì¶œë ¥:**
- `data/raw/search_logs.csv`
- `data/raw/frequency_distribution.png`

---

## Step 02: HEAD/TAIL ì¿¼ë¦¬ ì„ ì •

ê²€ìƒ‰ ë¡œê·¸ì—ì„œ ëŒ€í‘œ ì¿¼ë¦¬ë¥¼ ì„ ì •í•©ë‹ˆë‹¤.

```bash
python process/02.prepare_queries_and_fetch_os_results.py \
  --logs_csv data/raw/search_logs.csv \
  --output_dir data/processed \
  --head_sample_k 300 \
  --tail_sample_k 200
```

**ì¶œë ¥:**
- `data/processed/queries_head_300.csv`
- `data/processed/queries_longtail_200.csv`

---

## Step 03: OpenSearch ê²€ìƒ‰ ì‹¤í–‰

Lexicalê³¼ Semantic ê²€ìƒ‰ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.

### ì„¤ì • íŒŒì¼ ìˆ˜ì •

**`config/index_config.json`**
```json
{
  "indexes": {
    "baseline": {
      "name": "community-with-meta_classify-20250716",
      "board_filter": "MUZZIMA",
      "embedding_field": "vector_field"
    }
  }
}
```

**`config/search_experiments.json`**
```json
{
  "experiments": [
    {"id": "exp001", "name": "muzzima_lexical", "enabled": true},
    {"id": "exp002", "name": "muzzima_semantic", "enabled": true}
  ]
}
```

### ì‹¤í–‰ (ê¸°ì¡´ 3ë¶„í•  ì„¤ì • ì‚¬ìš©)

```bash
python process/03.fetch_opensearch_results.py \
  --run_only exp001 exp002
```

**ì¶œë ¥:**
- `data/search_results/exp001_head_*.csv` (Lexical HEAD)
- `data/search_results/exp001_tail_*.csv` (Lexical TAIL)
- `data/search_results/exp002_head_*.csv` (Semantic HEAD)
- `data/search_results/exp002_tail_*.csv` (Semantic TAIL)

---

## Step 04: Depth-K Pooling (HEAD+TAIL í†µí•©)

Lexicalê³¼ Semantic ê²°ê³¼ë¥¼ í•©ì³ ì „ì²´ ë¬¸ì„œ Poolì„ ë§Œë“­ë‹ˆë‹¤. ì´í›„ ë‹¨ê³„ëŠ” ì´ í†µí•© ì„¸íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ì§„í–‰ë©ë‹ˆë‹¤.

```bash
python process/04.pool_search_results.py \
  --results_head \
    data/search_results/exp001_head_*.csv \
    data/search_results/exp002_head_*.csv \
  --results_tail \
    data/search_results/exp001_tail_*.csv \
    data/search_results/exp002_tail_*.csv \
  --methods lexical semantic \
  --depth_k 20 \
  --query_set ALL \
  --output_dir data/pooled_results
```

**ì¶œë ¥:**
- `data/pooled_results/pooled_all_lexical_semantic_k20_*.csv`

**í†µê³„ ì˜ˆì‹œ:**
```
Pooling Statistics:
  Depth-K: 20
  Total unique documents: 6,000

  Documents found per method:
    lexical: 6,000 (100.0%)
    semantic: 6,000 (100.0%)

  Document overlap:
    Found by 1 method only: 3,600 (60.0%)
    Found by all methods: 2,400 (40.0%)
```

---

## Step 05: Poolì„ OpenSearchì— ì—…ë¡œë“œ (ë‹¨ì¼ ì¸ë±ìŠ¤)

í†µí•© pooled ê²°ê³¼ë¥¼ ë‹¨ì¼ ì¸ë±ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤.

```bash
python process/05.upload_pool_to_db.py \
  --pooled_csv data/pooled_results/pooled_all_lexical_semantic_k20_*.csv \
  --index_name search_relevance_judgments_all_20251101 \
  --delete_existing
```

**OpenSearch ì¸ë±ìŠ¤ êµ¬ì¡°:**
```json
{
  "mappings": {
    "properties": {
      "query": {"type": "keyword"},
      "doc_id": {"type": "keyword"},
      "query_set": {"type": "keyword"},
      "found_by_methods": {"type": "keyword"},
      "num_methods_found": {"type": "integer"},
      
      "lexical_rank": {"type": "integer"},
      "lexical_score": {"type": "float"},
      "semantic_rank": {"type": "integer"},
      "semantic_score": {"type": "float"},
      
      "TITLE": {"type": "text"},
      "CONTENT": {"type": "text"},
      "merged_comment": {"type": "text"},
      
      "relevance": {"type": "integer"},
      "labeled_by": {"type": "keyword"},
      "labeled_at": {"type": "date"},
      "notes": {"type": "text"}
    }
  }
}
```

**ì¶œë ¥:**
```
Summary
======================================================================
Index: search_relevance_judgments_head_20251101
Uploaded: 6,000 documents
Labeled: 0/6,000 (0.0%)
======================================================================
```

---

## Step 06: AI ê¸°ë°˜ Relevance Labeling (ë‹¨ì¼ ì„¸íŠ¸)

GPT-4ë¥¼ ì‚¬ìš©í•˜ì—¬ ìë™ìœ¼ë¡œ ê´€ë ¨ë„ë¥¼ í‰ê°€í•©ë‹ˆë‹¤. (OpenAI ê³µì‹ API ì‚¬ìš©)

```bash
python process/06.label_relevance.py \
  --index_name search_relevance_judgments_all_20251101 \
  --model gpt-4o-mini
```

**Labeling í”„ë¡œì„¸ìŠ¤:**
1. OpenSearchì—ì„œ unlabeled ë¬¸ì„œ ì¡°íšŒ
2. ê° ë¬¸ì„œì— ëŒ€í•´ GPT-4 í˜¸ì¶œ
3. Relevance (0/1/2) íŒì •
4. OpenSearch ì¸ë±ìŠ¤ì— ì—…ë°ì´íŠ¸

**ì¶œë ¥:**
```
Summary
======================================================================
Processed: 6,000
Successfully labeled: 5,950
Failed: 50
Labeling coverage: 5,950/6,000 (99.2%)

Relevance distribution:
  2 (Very relevant): 1,200 (20.2%)
  1 (Partially relevant): 2,400 (40.3%)
  0 (Not relevant): 2,350 (39.5%)
======================================================================
```

---

## Step 07: í‰ê°€ ì§€í‘œ ê³„ì‚° (all/head/tail ì œê³µ)

nDCG, Recall, MRR ë“±ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

```bash
# ALL
python process/07.calculate_metrics.py \
  --index_name search_relevance_judgments_all_20251101 \
  --methods lexical semantic \
  --k_values 5 10 20 \
  --subset all \
  --output_dir data/evaluation_results

# HEAD only
python process/07.calculate_metrics.py \
  --index_name search_relevance_judgments_all_20251101 \
  --methods lexical semantic \
  --subset head \
  --output_dir data/evaluation_results

# TAIL only
python process/07.calculate_metrics.py \
  --index_name search_relevance_judgments_all_20251101 \
  --methods lexical semantic \
  --subset tail \
  --output_dir data/evaluation_results
```

**ê³„ì‚°ë˜ëŠ” ì§€í‘œ:**
- **nDCG@K** (Normalized Discounted Cumulative Gain)
- **Recall@K** (ì¬í˜„ìœ¨)
- **Precision@K** (ì •ë°€ë„)
- **MRR** (Mean Reciprocal Rank)
- **MAP** (Mean Average Precision)

**ì¶œë ¥:**
```
Aggregated Metrics (Mean across queries):
----------------------------------------------------------------------
              ndcg@5  ndcg@10  ndcg@20  recall@5  recall@10  recall@20    mrr     map
lexical       0.4521   0.5234   0.5892    0.3421     0.5234     0.7123  0.5234  0.4891
semantic      0.4892   0.5621   0.6234    0.3892     0.5621     0.7456  0.5621  0.5234

Method Comparison:
----------------------------------------------------------------------

NDCG@20:
  ğŸ¥‡ semantic           : 0.6234
  ğŸ¥ˆ lexical            : 0.5892

RECALL@20:
  ğŸ¥‡ semantic           : 0.7456
  ğŸ¥ˆ lexical            : 0.7123

MRR:
  ğŸ¥‡ semantic           : 0.5621
  ğŸ¥ˆ lexical            : 0.5234
```

**íŒŒì¼ ì¶œë ¥:**
- `data/evaluation_results/head/per_query_metrics_lexical.csv`
- `data/evaluation_results/head/per_query_metrics_semantic.csv`
- `data/evaluation_results/head/aggregated_metrics.csv`

---

## Step 08: ê²°ê³¼ ì‹œê°í™”

í‰ê°€ ê²°ê³¼ë¥¼ ì°¨íŠ¸ì™€ ë¦¬í¬íŠ¸ë¡œ ìƒì„±í•©ë‹ˆë‹¤.

```bash
python process/08.visualize_results.py \
  --results_dir data/evaluation_results/head \
  --output_dir data/evaluation_results/head \
  --k_values 5 10 20
```

**ìƒì„±ë˜ëŠ” ì‹œê°í™”:**

1. **`method_comparison.png`** - ì£¼ìš” ì§€í‘œ ë¹„êµ (Bar chart)
2. **`metrics_heatmap.png`** - ì „ì²´ ì§€í‘œ íˆíŠ¸ë§µ
3. **`ndcg_by_k.png`** - Kê°’ì— ë”°ë¥¸ nDCG ë³€í™”
4. **`recall_by_k.png`** - Kê°’ì— ë”°ë¥¸ Recall ë³€í™”
5. **`distribution_ndcg@20.png`** - nDCG ë¶„í¬ (Violin plot)
6. **`distribution_recall@20.png`** - Recall ë¶„í¬
7. **`EVALUATION_REPORT.md`** - Markdown ë¦¬í¬íŠ¸

**ë¦¬í¬íŠ¸ ì˜ˆì‹œ:**
```markdown
# Search Evaluation Results Summary

## Overall Comparison

### Key Metrics

| Method | NDCG@10 | NDCG@20 | RECALL@10 | RECALL@20 | MRR | MAP |
|--------|---------|---------|-----------|-----------|-----|-----|
| lexical | 0.5234 | 0.5892 | 0.5234 | 0.7123 | 0.5234 | 0.4891 |
| semantic | 0.5621 | 0.6234 | 0.5621 | 0.7456 | 0.5621 | 0.5234 |

## Best Method per Metric

- **NDCG@10**: semantic (0.5621)
- **NDCG@20**: semantic (0.6234)
- **RECALL@20**: semantic (0.7456)
- **MRR**: semantic (0.5621)

## Performance Differences (vs. Baseline)

### NDCG@20

- âœ… **semantic**: 0.6234 (+0.0342, +5.8%)
```

---

## ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (í•œë²ˆì—)

```bash
#!/bin/bash

# Step 1: ê²€ìƒ‰ ë¡œê·¸ ìˆ˜ì§‘
python process/01.fetch_search_logs.py \
  --start_date 2024-05-01 \
  --end_date 2025-10-30

# Step 2: ì¿¼ë¦¬ ì„ ì •
python process/02.prepare_queries_and_fetch_os_results.py \
  --logs_csv data/raw/search_logs.csv

# Step 3: ê²€ìƒ‰ ì‹¤í–‰
python process/03.fetch_opensearch_results.py \
  --run_only exp001 exp002

# Step 4: Pooling (HEAD)
python process/04.pool_search_results.py \
  --results \
    data/search_results/exp001_head_*.csv \
    data/search_results/exp002_head_*.csv \
  --methods lexical semantic \
  --depth_k 20 \
  --query_set HEAD

# Step 5: OpenSearch ì—…ë¡œë“œ
python process/05.upload_pool_to_db.py \
  --pooled_csv data/pooled_results/pooled_head_*.csv \
  --index_name search_relevance_judgments_head_20251101 \
  --delete_existing

# Step 6: Relevance Labeling
python process/06.label_relevance.py \
  --index_name search_relevance_judgments_head_20251101

# Step 7: í‰ê°€ ì§€í‘œ ê³„ì‚°
python process/07.calculate_metrics.py \
  --index_name search_relevance_judgments_head_20251101 \
  --output_dir data/evaluation_results/head

# Step 8: ì‹œê°í™”
python process/08.visualize_results.py \
  --results_dir data/evaluation_results/head
```

---

## í‰ê°€ ì§€í‘œ í•´ì„

### nDCG@K (Normalized Discounted Cumulative Gain)
- **ë²”ìœ„**: 0.0 ~ 1.0
- **ì˜ë¯¸**: ìƒìœ„ Kê°œ ê²°ê³¼ì˜ í’ˆì§ˆ (ìˆœì„œ ê³ ë ¤)
- **í•´ì„**:
  - 1.0 = ì™„ë²½ (ëª¨ë“  ê´€ë ¨ ë¬¸ì„œê°€ ìƒìœ„ì—)
  - 0.5 ~ 0.7 = ì–‘í˜¸
  - < 0.5 = ê°œì„  í•„ìš”

### Recall@K
- **ë²”ìœ„**: 0.0 ~ 1.0
- **ì˜ë¯¸**: ì „ì²´ ê´€ë ¨ ë¬¸ì„œ ì¤‘ ìƒìœ„ Kê°œì— í¬í•¨ëœ ë¹„ìœ¨
- **í•´ì„**:
  - 0.7 = ê´€ë ¨ ë¬¸ì„œì˜ 70%ë¥¼ ì°¾ìŒ
  - ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ

### MRR (Mean Reciprocal Rank)
- **ë²”ìœ„**: 0.0 ~ 1.0
- **ì˜ë¯¸**: ì²« ë²ˆì§¸ ê´€ë ¨ ë¬¸ì„œê°€ ë‚˜íƒ€ë‚˜ëŠ” ìˆœìœ„ì˜ ì—­ìˆ˜
- **í•´ì„**:
  - 1.0 = ì²« ë²ˆì§¸ ê²°ê³¼ê°€ ê´€ë ¨ ë¬¸ì„œ
  - 0.5 = í‰ê·  2ë²ˆì§¸ì— ì²« ê´€ë ¨ ë¬¸ì„œ
  - 0.1 = í‰ê·  10ë²ˆì§¸ì— ì²« ê´€ë ¨ ë¬¸ì„œ

---

## ë¬¸ì œ í•´ê²°

### OpenSearch ì—°ê²° ì‹¤íŒ¨
```
âœ— OpenSearch connection failed
```
â†’ `.env` íŒŒì¼ì˜ OpenSearch ì„¤ì • í™•ì¸
  - OPENSEARCH_HOST
  - OPENSEARCH_ID (ë˜ëŠ” OPENSEARCH_USER)
  - OPENSEARCH_PW (ë˜ëŠ” OPENSEARCH_PASSWORD)

### Labeling ì‹¤íŒ¨
```
âœ— Labeling failed: Missing OPENAI_API_KEY
```
â†’ `.env` íŒŒì¼ì— `OPENAI_API_KEY` ì¶”ê°€

### ì§€í‘œ ê³„ì‚° ì‹¤íŒ¨
```
âœ— No results found for semantic
```
â†’ OpenSearch ì¸ë±ìŠ¤ì— `semantic_rank` í•„ë“œê°€ ìˆëŠ”ì§€ í™•ì¸
â†’ Step 5ì—ì„œ ì˜¬ë°”ë¥¸ pooled CSV ì‚¬ìš©í–ˆëŠ”ì§€ í™•ì¸
â†’ Step 6ì—ì„œ ë¼ë²¨ë§ì´ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸

---

## ì°¸ê³  ìë£Œ

- [TREC Evaluation](https://trec.nist.gov/)
- [nDCG ì„¤ëª…](https://en.wikipedia.org/wiki/Discounted_cumulative_gain)
- [ì •ë³´ ê²€ìƒ‰ í‰ê°€](https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-ranked-retrieval-results-1.html)
